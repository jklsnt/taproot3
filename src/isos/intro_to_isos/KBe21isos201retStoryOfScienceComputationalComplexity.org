#+TITLE: Story of Science: Computational Complexity Theory
#+CONTEXT: 21ISOS201
#+AUTHOR: Albert H
* sources
** gentle introductions
*** https://en.wikipedia.org/wiki/Computational_complexity_theory
*** https://complexityzoo.net/Petting_Zoo
*** https://www.simonsfoundation.org/2013/12/13/richard-karp/
* overview
** computational complexity theory studies how "difficult" a problem is
*** importantly, not how "good" an algorithm is... this field deals with all algorithms that solve a given problem
** key concepts
*** types of problems
*** Turing machines
*** reducibility
*** complexity classes
*** hierarchy
** key problems
*** P vs NP
* richard karp (the actual assignment)
** overview
*** basically, see https://www.simonsfoundation.org/2013/12/13/richard-karp/
** TODO storyboard richard karp life
* vocab and definitions                                                 :def:
** DTM, deterministic Turing machine
   A Turing machine with one infinite tape and a state function that has exactly one output for each (tape value, machine state) input. (Not a formal definition).
* complexity classes
** P
   Problems that can be solved in polynomial time using a deterministic Turing machine (DTM). These problems are generally considered "tractable" by the Cobham-Edmonds thesis.

   In practice, problems with large-degree solutions usually have smaller-degree solutions discovered later, so the division between P and other problems 'has turned out to be somewhat "natural"'.

   Examples: { graph reachability (whether two nodes are in the same component), 2SAT (SCC :)), matrix multiplication }
** NP
   Problems that can be checked "yes" in polynomial time. For example, graph isomorphism.
*** whats up with coNP? checkin "no"??                             :toexpand:
** NP-complete
   Problems that are complete for NP, aka. problems in NP that all other NP problems can be reduced to in polynomial time. One example is CircuitSAT: any NP problem can be reduced to CircuitSAT and any problem that CircuitSAT can be reduced to is also NP-complete. Showing that CircuitSAT or any other NP-complete problem can be solved in polynomial time implies that P = NP.

   Examples: TSP, SAT
** PH (polynomial hierarchy?)                                      :toexpand:
   Defined as the union of a set of recursively defined classes... something? Something about P and NP and oracles about NP and coNP. This is the thing that would collapse, I guess. Eg, why people don't think Graph Isomorphism is NP-complete.
** PSPACE
   Restricts space instead of time. This class is very large, and includes the entirety of NP (brute force check all possible proofs) and also PP and P^{#P}, apparently.

   Notable PSPACE-complete examples: QBF (or QSAT), deciding the winner of eg. Go.
** EXP
   run-time bounded by $2^{p(n)}$ where $p$ is a polynomial (ig). EXP is generally big enough: it contains PSPACE, the polynomial hierarchy (PH), and 'most problems we ever hope to attack'. Of course, there are bigger ones.
** AC^0, NC^0, NC
   small classes that have to do with circuit complexity.. generally the represent problems that can be solved quickly with massive parallelism.
** L
   Logarithmic space (logarithmic spaces on the Turing machine tape). This is nice because space is usually limited. L is contained within P because something about deterministic Turing machines?.
** P/poly
   polynomial time algorithms that solve a problem given an advice string which is at most polynomial in length and a function only of the input size. P/exp would make all problems trivial (provide a lookup table), but P/poly is actually interesting, ig. P/poly contains P, so $NP \notin P/poly$ would imply $P \neq NP$.
** BPP
   Randomized algorithms where the error rate is 'bounded by a constant'. Error rate could be improved by running the algorithm more times and taking a majority vote.

   AKS primality test made a previously randomized algorithm deterministic, which was a 'key example of derandomization'. People apparently think that P = BPP (and this is an important open problem).
* flows
** Wikipedia computational complexity theory
*** computational problems
**** problem instances
	 A problem describes the problem. the actual "numbers" that describe a specific problem is called a problem instance. sorting a list is a problem, sorting /this/ list is a problem instance.
**** representing problem instances
	 formally strings of characters from alphabets. The input size is the length of the string. Different representations can be chosen but it should be trivial (fast) to convert from one to the other.
**** decision problems (most basic type)
	 Generally, given an input, the output is either yes (accept) or no (reject). For example, deciding whether a graph is connected or not.
***** it can be thought of as a "formal language"                  :toexpand:
**** function problems
	 Very general: a function problem 'is a computational problem where a single output (of a total function) is expected for every input, but the output is more complex than that of a decision problem'. Basically calculate a non-binary function.

	 Examples: traveling salesman, integer factorization.

	 However, all function problems can be modeled as decision problems: For some function $f(*args) \to ans$, it can be modeled as the decision problem of whether $(*args, ans)$ is a valid output.
***** but does this really work? how can a decision TM be used to compute the function output efficiently? :toexpand:
**** size of an instance
	 Size is usually the length of the input. The complexity is a function of the input size, usually representing the worst case time or space (or any other complexity measure) required for any input size.
*** machine models and complexity measures
**** Turing machine
	 standard Turing machine stuff. its very general. Many types of turing machines (probabilistic, non-deterministic, quantum, etc) are used to define different complexity classes.
**** other machine models                                          :toexpand:
	 Other non-standard Turing machines are used, but the idea is that they aren't actually any better, somehow?
**** Complexity Measures
	 Usually time or space, but any complexity measure that satisfies Blum's complexity axioms can be used. Examples include: communication complexity, circuit complexity.

	 Also constant factors don't really matter. And its usually the worst case.

	 Importantly, complexity measures are also a function of the type of Turing machine used, since some Turing machines are better in some scenarios.
***** blums complexity axioms                                      :toexpand:
**** best/worst/average case
	 We generally talk about worst case complexity, but some algorithms have good average-case which is good enough (eg. quicksort). Generally, best-case < average-case < amortized analysis < worst-case.
**** upper and lower bounds for problems
	 Importantly, this is *not an upper or lower bound for an algorithm*. Instead, for problems in general, it's relatively easy to decide an upper bound (which is just the worst case complexity of any correct algorithm), but a lower bound is difficult (since it must involve algorithms that haven't been discovered yet).
*** complexity classes
**** dependencies
	 Complexity classes are a function of the following factors
***** problem type
	  { decision, function, counting, optimization, promise, etc }
***** computation model
	  { deterministic Turing machine, non-deterministic, Boolean circuits, quantum TM, monotone circuits, etc }
***** bounded resources
	  { polynomial time, logarithmic space, constant depth }
**** an example definition
	 #+begin_quote
	 The set of decision problems solvable by a deterministic Turing machine within time f(n). (This complexity class is known as DTIME(f(n)).)
	 #+end_quote

	 However, using a concrete function $f(n)$ is often computational-model-dependent, but the Cobham-Edmonds thesis states that 'the time complexities in any two reasonable general models of computation are polynomial related.'

	 This suggests that all if we want to be machine-independent, all polynomial problems are roughly the same and belong in the same class: P (for decision problems) and FP (for function problems).
***** why are there different classes if decision and function problems are the same-ish? dunno :toexpand:
**** important complexity classes
	 A nice list here but the complexity petting zoo is more friendly.
**** Hierarchy theorems                                            :toexpand:
	 We would like to establish a strict containment hierarchy within classes (but between different eg. polynomial functions). This does that, apparently?
**** Reduction
	 Many problems can be turned into other problems in their class, which provides an upper bound on the difficulty of the problem.

	 There are many types of reductions, but the most common type is the polynomial-time reduction which means the reduction takes polynomial time. If you take a non-polynomial reduction to turn a problem into a polynomial problem, then you haven't proven anything.
***** hardness and completeness
	  A problem $X$ is hard for a class $C$ if every problem in $C$ can be reduced to $X$. A problem $X$ is complete for $C$ if it is hard for $C$ it is in $C$. NP-complete problems are the "most difficult problems in NP" because other problems can be reduced to them.

	  Being able to reduce a hard problem to another problem shows that that other problem is just as hard, by contradiction. Similarly, being able to reduce a hard problem to a known easy one collapses the hierarchy.
*** important open problems
**** P vs NP
	 If any NP-complete problem can be reduced (polynomially) to a P problem, then many NP problems would be solvable in polynomial time. There are many NP problems that we would like to solve efficiently, so this would be a big deal.

	 In fact, many of the other 'important open problems' are important because they would show that $P \neq  NP$.
**** NP-indeterminate problems (in NP but not in P nor NP-complete) :toexpand:
	 some theorem shows that if P neq NP then there are NP-indeterminate problems. If we show that there are none, then that proves P = NP. Some unclassified problems (graph isomorphism problem, integer factorization problem) being NP-complete would 'collapse the polynomial hierarchy.' ?????
**** separations between other complexity classes
	 There are many classes that are improper subsets of each other. If any of those relations can be shown to be a proper subset, then classes on either side would be unequal. For example, many such relations exist between P and NP and showing that one of those relations is a proper subset relation would prove that P != NP. Or, proving that two classes (eg. P, PSPACE) are equal would squish all classes in between into one (in this case, showing that P = NP).
*** Intractability
	Meaning "not handleable". The Cobham-Edmonds thesis suggests that all polynomial problems are tractable. However, in the real world, specific numbers matter ($N^{15}$ is much worse than $0.0001^N$)
*** continuous complexity theory                                   :toexpand:
	Something about continuous functions or analog logic.
*** History
**** Many foundations laid, eg. Turing machine in 1936 which allowed for analysis of various algorithms.
**** First systematic study attributed to Juris Hartmanis and Richard E. Stearns in "On the Computational Complexity of Algorithms" (1965)
**** Edmonds (Cobham-Edmonds thesis) suggests polynomial problems are "good" (1965)
**** other studies of problems with bounded resources in the previous few years
**** Blum axioms for complexity measures (1967), and the "speed-up theorem"
**** 1971 Stephen Cook and Leonid Levin proved existance of practically relevant NP-complete problems
**** Richard Karp (1972) showed 21 relevant and NP-complete problems (op)
***** oldest of four children, born to jewish family in Dorchester, Boston
***** mother got harvard degree at age 57 and father wanted to go on to medschool after Harvard but became a math teacher bc he couldn't afford med school
***** went to harvard, some career jumps, mostly CS professor at UC Berkeley
***** leads a number of societies and organizations, including International Computer Science Institute, Simons Institute for the Theory of Computing
***** and also got a bunch of prizes for his work on NP-complete problems
***** Edmonds-Karp algorithm for max flow with Jack Edmonds 1971
***** landmark NP-completeness paper in 1972
****** showed 21 problems that SAT can be reduced to
****** a nice tree of reducibility + some previous work mentioned in the paper
***** 1973 Hopcroft-karp algorithm with john hopcroft for bipartite graph max matchings
***** some other work later on
